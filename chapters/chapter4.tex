\chapter{Generalizations}
\todo{Change chapter name}
In the previous chapter we presented some results about integer domains. In this one, we would like to explore how much of the previous results can be generalized to arbitrary domains.

In this chapter we assume a set $C$ of values and use as concrete domain $\pow(C)$. We also assume an under-approximation Galois insertion $\pow(C) (\alpha, id) A$, that is $\alpha \preceq \id_C$ and $A$ is closed by union (propositions \ref{ch2:th:under-gc-extensive-charact} and \ref{ch2:th:under-gc-union-closure}.

\todo{delete this, since it will be in the background chapter}
%As notation, we use $c, d$ for elements of $C$, $\bar{c}$ to denote some element of $C$ for which some pair $\{ c, \bar{c} \}$ is not representable (the specific pair is often clear from the context). We say an element of $\pow(C)$ (ie. a subset of $C$) is \textit{representable} if it belongs to $A$, or equivalently if $\alpha$ is the identity on it.
%It is worth noting that with this notation
%\[
%f^{\flat} = \alpha \circ f \circ \gamma = \alpha \circ f
%\]
%but they have ``different types": $\alpha \circ f$ is a function from $\pow(C)$ to $A$ and so can be applied to arbitrary elements of $\pow(C)$, while $f^{\flat}$ is from $A$ in itself, so it should always be applied to elements of $A$. This will be enforced putting and $\alpha$ in front of $\alpha \circ f$ whenever it's used as $f^{\flat}$.

Moreover, we often confuse a function from $C$ in itself and its additive extension, so if we say $f: C \rightarrow C$ is non emptying we actually mean that its additive extension
\begin{align*}
	f: \pow(C) &\rightarrow \pow(C)	\\
	S & \mapsto \{ f(c) \svert c \in S \}
\end{align*}
is non emptying.
% CHECK WHAT TO KEEP

\section{Non relational domains}

An important notion for abstract domains is that of being or not relational. Intuitively, a non relational domain can't capture relations between different variables. For instance, the product of interval domains (see example \ref{ch2:ex:intervals}) for each variable is non relational, since it only consider a single variable.
\begin{example}
	Consider the simple program fragment
	\begin{minted}{C}
		y = 5 - x;
		z = x + y;
	\end{minted}
	and assume at the beginning the variable \code{x} assumes values in the interval $[0, 5]$. An interval analysis on this fragment would find that \code{y} takes values in the interval $5 - [0, 5] = [0, 5]$, and then \code{z} is in the result of $[0, 5] + [0, 5]$, that in general is $[0, 10]$. However at the end of the program \code{z} is always $5$.
	The issue here is that the values of \code{x} and \code{y} aren't independent, so an operation between these two variables can't actually have all possible inputs with \code{x} in $[0, 5]$ and \code{y} in $[0, 5]$, but only those that satisfies the \textit{relation} \code{y = 5 - x}. However, the interval domain knows nothing about this relation since it abstracts each variable independently.
\end{example}
There exists extensions of the intervals domain that are relational, for instance octagons (\cite{mine-octagons}) and polyhedrons (\cite{cousot-polyhedrons}). In general, non relational domains are less precise but also computationally less expensive.

The concept of non relational domain is frequently referred to in the literature, but not so often formalized. We stick to a slight variation of the one proposed in \cite{giacobazzi-analyzing-analyses} (section 5.2), that is quite simple but still sufficient for our goals.
Given a set of variables $\text{Vars}$ and, for each one of these variables $x$, a concrete domain $C_x$, a concrete domain for the set of variables is the power set of
\[
C = \prod\limits_{x \in \text{Vars}} C_x
\]
An element of the cartesian product $C$ is a function that maps each variable $x$ into a value in its domain $C_x$ (a tuple of elements, one for each variable $x$ and taken from $C_x$), so it's exactly one of the possible states a program that operates on the set of variables $\text{Vars}$ can be in. The power set of that product is hence a concrete domain whose elements are set of possible states of such a program.

\begin{definition}[Non relational domain]
	An abstract domain $A$ for the concrete domain $\pow(C)$ is \textit{non relational} if it can be decomposed as
	\[
	A = \prod\limits_{x \in \text{Vars}} A_x
	\]
	that is a product of one abstract domain for each variable, where for each variable $x$ there is a Galois connection $\pow(C_x) (\alpha_x, \gamma_x) A_x$ between the concrete domain for that variable and $A_x$, and the Galois connection $\pow(C) (\alpha, \gamma) A$ is defined by
	\begin{align*}
		\alpha(S) &= (\alpha_x(\pi_x(S)), \alpha_y(\pi_y(S)), \dots) \\
		\gamma(a) &= \gamma_x(a_x) \times \gamma_y(a_y) \times \dots
	\end{align*}
	where $\pi_x$ denotes the additive extension of the projection $C \rightarrow C_x$ and $a_x$ is the $x$ component of the tuple $a \in \prod\limits_{x \in \text{Vars}} A_x$.
\end{definition}

A non relational abstract domain is a tuple of elements, one for each variable $x$, and describes the set of concrete states where each variable belongs to the values in it's abstract coordinate. The abstraction is performed on each variable independently, projecting all states in $S$ on that variable and then abstracting the resulting set. The concretization is performed on each variable independently, and then results are multiplied in all possible ways to get the concrete element.
This separate handling of different variables prevent expressing relations. When abstracting, we lose all informations on possible relations since we project on each variable before abstracting. Conversely, when concretizing an abstract tuple, each element is concretized independently and hence the result is a Cartesian product of sets of possible values for each variable.

\todo{talk about Cartesian abstractions? Probably not}%Non relational domains are a special case of so-called Cartesian domains (\todo{ref?}): first we abstract

If we consider and under-approximation non relational domain, we have to take into account both the fact that abstract elements are concretized in Cartesian product of set of values for each variable, and the fact that the domain is closed under union. Unfortunately, these two properties are incompatible.
\begin{prop}\label{ch3:th:underapprox-non-rel}
	Let $\pow(C) (\alpha, \gamma) A$ be an under-approximating Galois connection, and assume $A$ is non relational. Then, for all but one variable (let it be $y$), the abstract domain $A_x$ is \textit{almost} trivial, that is it contains at most one value $a_x$ other than $\bot$:
	\[
	\forall x \in \text{Vars} \setminus \{ y \} \ .\ A_x = \{ a_x, \bot \}
	\]
\end{prop}
\begin{figure}[!ht]
	\centering{
		{
			\fontsize{11pt}{13pt}\selectfont
			\def\svgwidth{3.5in}
			\input{images/non-rel-proof.pdf_tex}
		}
		\caption{Intuition of the proof of proposition \ref{ch3:th:underapprox-non-rel}}
		\label{ch3:fig:rel-domain}
	}
\end{figure}
\begin{proof}
	For the sake of simplicity, let us assume that the Galois connections $\pow(C_x) (\alpha_x, \gamma_x) A_x$ for each variable $x$ are actually Galois insertions, and let us use the usual identification of $A_x$ and $\gamma(A_x) \subseteq \pow(C_x)$.
	Moreover, let us assume that $\text{Vars}$ only contains two variables; the proof generalizes straightforwardly to an arbitrary set of variables, but it clutters the notation a lot.

	By way of contradiction, suppose that for two distinct variables $y$ and $z$ domains $A_y$ and $A_z$ contains more than one element other than $\bot$: let $S_y, S'_y \in A_y$ and $S_z, S'_z \in A_z$.

	Since $S_y \neq S'_y$, without loss of generality we can assume $S'_y \nsubseteq S_y$ (otherwise just swap their names), and let $T_y = S_y \cup S'_y$. By union closure of the abstract domain $A_y$ (proposition \ref{ch2:th:under-gc-union-closure}), $T_y \in A_y$. Moreover, since $S'_y$ is not a subset of $S_y$, $T_y$ is a strict superset of $S_y$, and $S_y \neq \emptyset$ by hypothesis:
	\[
	\emptyset \subset S_y \subset T_y
	\]
	The situation is described on the horizontal axis of figure \ref{ch3:fig:rel-domain}: the non empty set $S_y$ is a strict subset of $T_y$, and hence the difference of the two sets is non empty too.
	Analogously, we define $T_z = S_z \cup S'_z$ and get that $T_z \in A_z$ and
	\[
	\emptyset \subset S_z \subset T_z
	\]

	We now use union closure of the domain $A$ to get that
	\[
	S_y \times T_z \cup T_y \times S_z
	\]
	is in the abstract domain too. But this is not the product of two subsets of $C_y$ and $C_z$, as can be be quickly seen in figure \ref{ch3:fig:rel-domain}: the union is the red area, that clearly isn't a rectangle (a product of sets for $y$ and $z$).

	Formally, consider $v_y \in S_y$, $w_y \in T_y \setminus S_y$, $v_z \in S_z$ and $w_z \in T_z \setminus S_z$. We have that $(w_y, v_z) \in T_y \times S_z$ and $(v_y, w_z) \in S_y \times T_z$ but $(w_y, w_z) \notin S_y \times T_z \cup T_y \times S_z$ since it doesn't belong to neither of the two, so this can't be the product of two subsets of $C_y$ and $C_z$.
\end{proof}

\section{Non emptyingness}
\todo{copy definitions/lemmas here?}
In the previous chapter we introduced the definition \ref{ch3:def:non-emptying} of non emptying function when talking about integers, but this notion is fully general with respect to the concrete domain: we already applied it to both the infinite set of integers $\setZ$ and the finite interval $[-N, N]$, so we want now to work with an arbitrary concrete domain $\pow(C)$.
We remark that also definition \ref{ch3:def:repr-with-set} of elements representable with a set and lemmas \ref{ch3:th:f-non-repr-pair} and \ref{ch3:th:R-S-bound-integer-inf} are fully general and doesn't depend on the specific integer domain we used in the previous chapter. Hence we start from these to build results independent of the concrete domain.

In our results for integers, we showed that an under-approximation abstract domain can't be non emptying for all functions in a given family (in that case, adding a constant). Now we would like to generalize those results, investigating which conditions \textit{on the family of functions} are sufficient to ensure that no abstract domain can be non emptying for all of them. The reason of this is that first we fix the function family we want to analyse (corresponding to the program at hand) and then we look for a suitable domain for that specific family we fixed.

In the following sections, we present two impossibility results. They share the same general structure: we fix a function family $F$ with some properties, assume that they are all non emptying in the abstract domain and that $R$ isn't empty. We need this last assumption in order to use lemma \ref{ch3:th:f-non-repr-pair}: if $R$ is empty we can't prove $f(\bar{c}) \in R$.
Both results require the function family to satisfy a specific condition: that of being highly surjective.

\begin{definition}[Highly surjective function family]\label{ch4:def:highly-onto-func-family}
	Given a family $F$ of functions from $C$ in itself and an element $c \in C$, let
	\[
	P(c) = \{ d \in C \svert \exists f_d \in F .\ f_d(d) = c \}
	\]
	be the set of \textit{preimages of $c$}, elements of $C$ for which there exists a function in $F$ that maps that element to $c$.

	We say that the family $F$ is \textit{highly surjective} (or \textit{highly onto}) if $P(c)$ is infinite for any possible choice of $c \in C$.
\end{definition}

Clearly the family of functions that add a constant is highly surjective, and actually satisfies a stronger property: for any possible choice of $c$ \textit{all} possible choices of $d$ (that are infinitely many) have a function $f_d(x) = x + c - d$ such that $f_d(d) = c$, that is $P(c) = \setZ$. We used this property in the proof in order to apply lemma \ref{ch3:th:f-non-repr-pair}: we took an element $\bar{n}$ not representable with $n_0$ and used said function in order to get $f(\bar{n}) = n_0 \in R$, hence being able to apply the lemma to get $f(n_0) \in R$. The point is that at the beginning the only representable element we know of is $n_0$, so our only hope to show $f(\bar{n}) \in R$ is to show that $f(\bar{n}) = n_0$, hence we need a function that maps $\bar{n}$ in $n_0$.

The following two sections present the two impossibility results, while the last one of the chapter discuss limits of approaches based on non emptyingness.

\section{First result}
\todo{change section name}
This first result is somehow more ``local", in the sense that we require each function in $F$ to satisfy some properties ``on its own", independently from other functions in $F$. In its basic formulation, we just require each function in $F$ to be injective.

\begin{theorem}\label{ch4:th:non-empt-res-local-basic}
	Let $F$ be a family of functions from $C$ in itself such that
	\begin{itemize}
		\item $F$ is highly surjective
		\item all functions $f \in F$ are injective
	\end{itemize}
	Assume also that $R$ isn't empty. Then $A$ can't be non emptying for all $f \in F$.
\end{theorem}
The proof of this theorem is quite involved. It starts from $c_0 \in R$, that exists because $R$ is not empty, and iteratively creates a sequence $c_n$ of representable elements. This yields a contradiction since $R$ should be finite.

\begin{figure*}[ht]
	\centering
	\begin{tikzpicture}
		\tikzset{vertex/.style = {shape=circle,fill,inner sep=1pt}}
		\tikzset{func/.style = {->,> = latex'}}

		% vertices \bar{c}
		\node[vertex,label=left:$\bar{c}$] (bc1) at (0,1){};
		\node[vertex,label=left:$\bar{c}$] (bc2) at (0,0.5){};
		\node at (0,0.1){$\vdots$};
		\node[vertex,label=left:$\bar{c}$] (bc3) at (0,-0.5){};
		\node[vertex,label=left:$\bar{c}$] (bc4) at (0,-1){};

		% vertices cn
		\node[vertex,label=above:$c_0$] (c0) at (1,0){};
		\node[vertex,label=above:$c_1$] (c1) at (2,0){};
		\node[vertex,label=above:$c_2$] (c2) at (3,0){};
		\node (c2dots) at (4,0){$\dots$};
		\node[vertex,label=above:$c_n$] (cn) at (5,0){};
		\node (cndots) at (6,0){$\dots$};

		%edges \bar{c} -> c_0
		\draw[func] (bc1) to (c0);
		\draw[func] (bc2) to (c0);
		\draw[func] (bc3) to (c0);
		\draw[func] (bc4) to (c0);
		% edge c0 -> c1 -> ...
		\draw[func] (c0) to (c1);
		\draw[func] (c1) to (c2);
		\draw[func] (c2) to (c2dots);
		\draw[func] (c2dots) to (cn);
		\draw[func] (cn) to (cndots);
	\end{tikzpicture}
	\caption{Graphical representation of the ``final" $f$}
	\label{ch4:fig:final-f-sketch}
\end{figure*}

The main idea is to pick a suitable $f$ in $F$ and define the sequence as the iterates $c_{n+1} = f(c_n)$. This function is sketched in figure \ref{ch4:fig:final-f-sketch}. The initial set of $\bar{c}$ mapped to $c_0$ is required to be able to apply lemma \ref{ch3:th:f-non-repr-pair} to all pairs $\{ \bar{c}, c_n \}$ and get that $c_{n+1} = f(c_n)$ is representable, since $f(\bar{c}) = c_0 \in R$.
The issue with this idea is that we don't have enough information on the sequence to pick such an $f$ at the beginning, so we actually bring along a (huge) list of candidate $f$ that all coincides on a prefix of the sequence. At step $n$, we pick a new element of the sequence among the possible images of $c_{n-1}$ through all candidate $f$ we have at that point, and discard all those that doesn't match the choice.

Actually, instead of directly consider functions, we represent them with elements $\bar{c}$ of $C$. Each element represent a function $f_{\bar{c}}$ that satisfies $f_{\bar{c}}(\bar{c}) = c_0$. Note that this function exists for ``enough" (that is, infinitely many) $\bar{c}$ because of the high surjectivity hypothesis. We call $E_n$ the set of $\bar{c}$ that represent functions that are ``valid" for the prefix up to $n$, ie. they map $c_{i}$ to $c_{i+1}$ for $0 \le i \le n - 1$. The core of the proof is an induction that proves that $E_n$ always contains infinitely many elements and that the newly chosen $c_n$ is different from all the others.

\begin{proof}
	Assume by contradiction that $A$ is non emptying for all $f \in F$. By hypothesis, $R$ isn't empty and thus we can take a $c_0 \in R$. We then construct recursively a sequence of representable elements $c_n$.

	Given an element $c \in C$, let
	\[
	NR(c) = C \setminus R(c) = \{ \bar{c} \in C \svert \{ c, \bar{c} \} \text{ is not representable} \}
	\]
	be the set of elements that are \textit{not} representable with it.

	Since $F$ is an highly surjective family, for an infinite amount of $\bar{c}$ there exists $f_{\bar{c}}$ such that $f_{\bar{c}}(\bar{c}) = c_0$.

	To ease presentation, we define here $E_n$, a set of element of $C$ that depends on the sequence $c_n$ we'll construct in the proof. Do note that the definition of $E_n$ only depends on elements of the sequence up to $c_n$.
	\begin{align*}
		E_n = \{\bar{c} \in C \svert & \forall\ 0 \le i \le n \ .\ \bar{c} \in NR(c_i), \\
		& f_{\bar{c}}(\bar{c}) = c_0, \\
		& \forall\ 0 \le i \le n - 1 \ .\ f_{\bar{c}}(c_i) = c_{i + 1} \}
	\end{align*}
	In the light of the introduction above, the first line is needed to apply lemma \ref{ch3:th:f-non-repr-pair} to the pair $\{ c_i, \bar{c} \}$ and get that $f_{\bar{c}}(c_i) = c_{i+1}$ is representable. The second one means that $\bar{c}$ actually represents $f_{\bar{c}}$, and the last is the requirement that $f_{\bar{c}}$ coincides on the prefix of the sequence up to $c_n$.

	We observe that the sequence $E_n$ can also be defined inductively by
	\begin{align*}
		E_0 &= \{ \bar{c} \in C \svert \bar{c} \in NR(c_0), f_{\bar{c}}(\bar{c}) = c_0 \} \\
		&= NR(c_0) \cap P(c_0)
	\end{align*}
	\begin{align*}
		E_{n+1} &= \{\ \bar{c} \in E_n \svert \bar{c} \in NR(c_{n+1}), f_{\bar{c}}(c_n) = c_{n+1} \} \\
		&= NR(c_{n+1}) \cap \{\ \bar{c} \in E_n \svert f_{\bar{c}}(c_n) = c_{n+1} \} \addtocounter{equation}{1}\tag{\theequation} \label{ch4:eq:En-relation}
	\end{align*}
	$E_0$ is the intersection of $NR(c_0)$ and the set of $\bar{c}$ for which there exists $f_{\bar{c}}$. Using lemma \ref{ch3:th:R-S-bound-integer-inf} to say $R(c_0)$ is finite and recalling that $P(c_0)$ is infinite (by high surjectivity), we observe that
	\begin{align*}
		E_0 = P(c_0) \cap NR(c_0) = P(c_0) \setminus (C \setminus NR(c_0)) = P(c_0) \setminus R(c_0)
	\end{align*}
	is infinite too.

	We then prove by induction on $n$ the following three statements: first $c_n$ is representable, ie. $c_n \in R$; second $E_n$ is infinite; third $c_n$ is different from all $c_i$ for $0 \le i \le n - 1$.
	We've already proved base case for $n = 0$: $c_0$ is representable by hypothesis, $E_0$ is infinite as shown above and the third condition is vacuous since there are no $0 \le i \le -1$.
	For the inductive step, assume the three hypothesis hold for $n$ and let us prove them for $n + 1$.
	Consider the set
	\[
	S = \{ f_{\bar{c}}(c_n) \svert \bar{c} \in E_n \}
	\]
	of candidate $c_{n+1}$.
	Since $c_n \in R$, $\bar{c} \in E_n \subseteq NR(c_n)$ and $f_{\bar{c}}(\bar{c}) = c_0 \in R$ (by inductive hypotheses) we can apply lemma \ref{ch3:th:f-non-repr-pair} to get $f_{\bar{c}}(c_n) \in R$ too, hence $S$ is a subset of $R$.
	Since $R$ is finite also $S$ must be, and by inductive hypothesis we know $E_n$ is infinite, so there must exists an element $c_{n+1}$ in $S$ such that an infinite amount of $\bar{c} \in E_n$ satisfies $f_{\bar{c}}(c_n) = c_{n+1}$. We observe that, as shown above, $c_{n+1} \in R$. Moreover we chose $c_{n+1}$ such that
	\[
	\{\ \bar{c} \in E_n \svert f_{\bar{c}}(c_n) = c_{n+1} \}
	\]
	is infinite, so we get
	\[
	\{\ \bar{c} \in E_n \svert f_{\bar{c}}(c_n) = c_{n+1} \} \cap NR(c_{n+1}) = \{\ \bar{c} \in E_n \svert f_{\bar{c}}(c_n) = c_{n+1} \} \setminus R(c_{n+1})
	\]
	is infinite too because $R(c_{n+1})$ is finite. But this, by equation \eqref{ch4:eq:En-relation} above, is exactly $E_{n+1}$.

	We only have to show that $c_{n+1} \neq c_i$ for all $0 \le i \le n$. Assume by contradiction that this is not the case: for some $0 \le j \le n$ it holds $c_{n+1} = c_j$, and let us distinguish two cases. If $j = 0$ we get $f_{\bar{c}}(c_n) = c_{n+1} = c_0 = f_{\bar{c}}(\bar{c})$, that would imply $c_n = \bar{c}$, but the former is representable and the latter is not, absurd. If otherwise $j > 0$ we get $f_{\bar{c}}(c_n) = c_{n+1} = c_j = f_{\bar{c}}(c_{j-1})$, that would imply $c_n = c_{j-1}$, that is not the case by inductive hypothesis. So $c_{n+1} \neq c_j$, and this concludes the inductive proof.

	By the induction above we have that all $c_n$, that are an infinite amount, are elements of $R$ and are all distinct. This yields the desired contradiction because $R$ is finite.
\end{proof}

Intuitively, if we were able to find the ``final" $f$ from the beginning, it would look like figure \ref{ch4:fig:final-f-sketch}: some $\bar{c}$, those for which that $f$ is $f_{\bar{c}}$, are mapped to $c_0$, and from there it just maps each $c_n$ to $c_{n+1}$. Do note that this is just an intuitive description: in fact, such an $f$ may not even exists (this correspond to the limit of $E_n$ being empty), but is indeed useful to visualize the proof.

A simple example of such a function family are constant sums over integers.
\begin{example}
	We take $C = \setZ$ and
	\[
	F = \{ \lambda x. x + n \svert n \in \setZ \}
	\]
	The family is highly surjective (actually $P(c) = \setZ$ for all $c$) and all these functions are injective, so it meets the hypothesis of the theorem.
\end{example}

Even though we've already proved that no abstract domain can be non emptying for all functions in this family $F$ in proposition \ref{ch3:th:ne-sum-nonexsistence-inf} in the previous chapter, it's important to note that this proof isn't a generalization of the proof of that proposition. In this proof, we iterate a single $f$ to build the entire sequence, while in that one we change the function every time, mapping the non representable $\bar{n}$ to the newly found representable $n_0 + t d$ to get that the image of $n_0$ through that function is representable too, as sketched in figure \ref{ch4:fig:ne-sum-inf-sketch}.

\begin{figure*}[ht]
	\centering
	\begin{tikzpicture}
		\tikzset{vertex/.style = {shape=circle,fill,inner sep=1pt}}
		\tikzset{func/.style = {->,> = latex'}}

		% nodes cn
		\node[label=above:$\bar{n}$] (bnt) at (0, 2){};
		\node (bnb) at (0, -2){};
		\draw (bnt) to (bnb);

		\node[label=above:$n_0$] (n0t) at (2, 2){};
		\node (n0b) at (2, -2){};
		\draw (n0t) -- (n0b);

		\node[label=above:$n_0+d$] (n1t) at (4, 2){};
		\node (n1b) at (4, -2){};
		\draw (n1t) -- (n1b);

		\node[label=above:$n_0+2d$] (n2t) at (6, 2){};
		\node (n2b) at (6, -2){};
		\draw (n2t) -- (n2b);

		\node[label=above:$n_0+3d$] (n3t) at (8, 2){};
		\node (n3b) at (8, -2){};
		\draw (n3t) -- (n3b);

		\node[label=above:$n_0+4d$] (n4t) at (10, 2){};
		\node (n4b) at (10, -2){};
		\draw (n4t) -- (n4b);

		% f_0
		\node[label=left:$f_d$] at (0, 1.5){};
		\draw[func] (0, 1.5) to [bend left=10] (2, 1.5);
		\draw[func] (2, 1.5) to [bend left=10] (4, 1.5);

		% f_1
		\node[label=left:$f_{2d}$] at (0, 0.5){};
		\draw[func] (0, 0.5) to [bend left=10] (4, 0.5);
		\draw[func] (2, 0.5) to [bend left=10] (6, 0.5);

		% f_2
		\node[label=left:$f_{3d}$] at (0, -0.5){};
		\draw[func] (0, -0.47) to [bend left=10] (6, -0.47);
		\draw[func] (2, -0.53) to [bend left=10] (8, -0.53);

		% f_3
		\node[label=left:$f_{4d}$] at (0, -1.5){};
		\draw[func] (0, -1.47) to [bend left=10] (8, -1.47);
		\draw[func] (2, -1.53) to [bend left=10] (10, -1.53);
	\end{tikzpicture}
	\caption{Graphical representation of the proof of proposition \ref{ch3:th:ne-sum-nonexsistence-inf}}
	\label{ch4:fig:ne-sum-inf-sketch}
\end{figure*}

Another example are rational or real numbers, with sums or products, as shown for instance in this example
\begin{example}
	We take $C = \mathbb{Q} \setminus \{ 0 \}$ and
	\[
	F = \{ \lambda x. x \cdot q \svert q \in \mathbb{Q} \setminus \{ 0 \} \}
	\]
	The family is highly surjective since $P(c) = \mathbb{Q} \setminus \{ 0 \}$ for all $c$, and all these functions are invertible, hence injective.
\end{example}

An interesting observation about the proof is that we used injectivity of $f$ only to show that $c_{n+1} \neq c_i$, so any other condition that allows us to do so is a fine alternative. As an example, we present here the possibility that $f$ is acyclic instead of injective.
\begin{definition}[Acyclic function]
	Given a function $f$ from $C$ in itself we say that it's \textit{acyclic} if, for any finite sequence $c_0, c_1, \dots c_n$ of elements of $C$ of any length, it doesn't happen that
	\[
	f(c_0) = c_1, f(c_1) = c_2, \dots f(c_n) = c_0
	\]

	Is this holds for all sequences of length at least $2$, we say $f$ is \textit{almost acyclic}.
\end{definition}
If $f$ is acyclic the theorem holds: if for some $0 \le j \le n$ it were $c_{n+1} = c_j$, we would incur in the contradiction that $f$ has the cycle
\[
f(c_j) = c_{j+1}, \dots, f(c_n) = c_{n+1} = c_j
\]

However acyclicness is quite restrictive because it also considers sequences of length $1$, that means that $f$ can't have fixpoints. Almost acyclicness is useful exactly for this, but it's not enough on its own.
In particular, we can require almost acyclicness to functions in $F$ if we're also able to guarantee that $f(c_n) = c_{n+1} \neq c_n$, because for $0 \le j \le n - 1$ the same proof as above is still correct. A possible condition to enforce this is
\[
\forall c \in C.\ f(c) \neq c \implies f(f(c)) \neq f(c)
\]
This condition is enough since all $c_n$ are the image of something through $f$: for $n = 0$, $f(\bar{c})= c_0 \neq \bar{c}$, and for $n > 0$, $f(c_{n-1}) = c_n \neq c_{n-1}$.
This condition could be equivalently restated as any non-fixpoint of $f$ can't be mapped to a fixpoint, that is
\[
f(C \setminus \fix(f)) \subseteq C \setminus \fix(f)
\]
This condition is still restrictive, but is less so than acyclicness, that prevents fixpoints to exist at all.
Theorem \ref{ch4:th:non-empt-res-local-basic} could hence be generalized as
\begin{theorem}\label{ch4:th:non-empt-res-local}
	Let $F$ be an highly surjective function family from $C$ in itself such that all functions $f \in F$ satisfies at least one of the following conditions:
	\begin{itemize}
		\item $f$ is injective
		\item $f$ is acyclic
		\item $f$ is almost acyclic and $f(C \setminus \fix(f)) \subseteq C \setminus \fix(f)$
	\end{itemize}
	Assume also that $R$ isn't empty. Then $A$ can't be non emptying for all $f \in F$.
\end{theorem}

As an example of this more general version, we propose floating-point numbers and multiplications.
\begin{example}\label{ch4:ex:fp-numbers-local}
	We fix as concrete domain $C$ the set $\mathcal{F_+}$ of strictly positive floating-point numbers that can be represented with a fixed number of significant digits ($t$ bits) but with an arbitrary precision exponent. We make this choice in order to preserve characteristics of floating-point arithmetic but to have an infinite domain; a finite number of bits for exponents too would require a theorem for finite domains.\todo{Will I talk about this in a later section?} We limit ourselves to positive numbers since multiplications don't change signs, a discussion considering also negative numbers is completely analogous but takes twice as long to handle details of sign changes.

	Let $\cdot$ and $\odot$ denote respectively real product and its floating-point arithmetic approximation\todo{add ref}, and consider the function family
	\[
	F = \{ \lambda x . x \odot y \svert y \in \mathcal{F_+} \}
	\]

	First, we recall some basic properties about floating-point numbers and arithmetic. A floating-point number is defined by a significant $s$, of $t$ bits and satisfying $1 \le s < 2$, and an exponent $p$, and these correspond to the number $s \cdot 2^p$. If $\macheps$ is the machine epsilon and $\tilde{x}$ denotes the floating-point approximation of the real number $x$, then
	\[
	\left\lvert \frac{\tilde{x} - x}{x} \right\rvert < \macheps
	\]
	The smallest floating-point number greater than $1$ is $1 + 2^{-t+1}$. Analogously, the greatest floating-point number smaller than $1$ is $1 - 2^{-t}$. Moreover, $\macheps \le 2^{-t+1}$, hence the greatest number smaller than $1$ is at most $1 - \macheps / 2$.
	The floating-point multiplication is defined as\todo{fix the tilde because it's ugly}
	\[
	x \odot y = \widetilde{(x \cdot y)}
	\]
	and it is commutative and satisfies the following two properties.
	For any three floating point numbers $x$, $y$ and $z > 0$, if $x \le y$ also $x \odot z \le y \odot z$.
	For any two floating point numbers $x = s_1 \cdot 2^{p_1}$ and $y = s_2 \cdot 2^{p_2}$, the product $x \odot y = (s_1 \odot s_2) \cdot 2^{p_1 + p_2}$.

	We now show that $F$ meets the hypothesis of the theorem.
	The function family is highly surjective since, fixed $x = s \cdot 2^p$, we have that for all $n \ge 0$ the number
	$x \cdot 2^{-n} = s \cdot 2^{p-n}$ is in $\mathcal{F_+}$ and
	\[
	(x \cdot 2^{-n}) \odot (1 \cdot 2^{n}) = x
	\]
	hence $P(x) \supseteq \{ 1 \cdot 2^{-n} \svert n \ge 0 \}$ is infinite.
	For the second condition, if $y = 1 \cdot 2^{0}$ we have that the function $\lambda x. x \odot y$ is the identity, hence injective. So assume $y = s \cdot 2^p \neq 1 \cdot 2^0$ and let us show that $\lambda x. x \odot y$ is acyclic. Assume by contradiction it has a cycle $f(x_0) = x_1, f(x_1) = x_2, \dots, f(x_n) = x_0$. By monotonicity we have
	\[
	f(x) = x \odot y \ge x \odot 1 = x
	\]
	and then
	\[
	x_0 \le f(x_0) = x_1 \le f(x_1) = x_2 \le \dots \le x_n \le f(x_n) = x_0
	\]
	thus that all the elements of the cycle are equal, in particular $f(x_0) = x_0$. So we only have to show that such a function can't have a fixpoint.

	To do so, we distinguish four cases on $y = s \cdot 2^p$. In all four we assume $x_0 = s_0 \cdot 2^{p_0}$.
	If $p \ge 1$, since $s \ge 1$ we have
	\begin{align*}
		x_0 \odot y &= (s_0 \odot s) \cdot 2^{p_0 + p} \\
		&\ge (s_0 \odot 1) \cdot 2^{p_0 + 1} \\
		&= s_0 \cdot 2^{p_0} \cdot 2 > x_0
	\end{align*}
	If $p \le -2$, since $s < 2$ we have
	\begin{align*}
		x_0 \odot y &\le (s_0 \odot 2) \cdot 2^{p_0 + p} \\
		&\le (s_0 \odot 1) \cdot 2^{p_0 + p + 1} \\
		&= s_0 \cdot 2^{p_0} \cdot 2^{p + 1} \le x_0 / 2\\
	\end{align*}
	If $p = -1$ it must be the case that $s \cdot 2^{-1} \le 1 - \macheps / 2$, that is $s \le 2 - \macheps$, thus
	\[
	x_0 \odot y \le (s_0 \odot (2 - \macheps)) \cdot 2^{p_0 - 1}
	\]
	If $s_0 \odot (2 - \macheps) \ge s_0$ we would have
	\begin{align*}
		\macheps &> \left\lvert \frac{(s_0 \odot (2 - \macheps)) - s_0 \cdot (2 - \macheps)}{s_0 \cdot (2 - \macheps)} \right\rvert \\
		&\ge \left\lvert \frac{s_0 - s_0 \cdot (2 - \macheps)}{s_0 \cdot (2 - \macheps)} \right\rvert \\
		&= \left\lvert \frac{- 1 + \macheps}{2 - \macheps} \right\rvert \\
		&= \frac{1 - \macheps}{2 - \macheps} \ge \frac{1}{2} \ge \macheps\\
	\end{align*}
	where we assumed $\macheps \le 1/2$ for the last line, and this is a contradiction.
	Lastly, let us consider the case of $p = 0$. Since $y \neq 1$ it must be the case that $s \neq 1$ too, hence $s \ge 1 + 2^{-t+1}$. From this it follows
	\begin{align*}
		x_0 \odot y &= (s_0 \odot s) \cdot 2^{p_0 + p} \\
		&\ge (s_0 + s_0 \cdot 2^{-t+1}) \cdot 2^{p_0}
	\end{align*}
	Since $s \ge 1$ we have that $s_0 \cdot 2^{-t+1} \ge 2^{-t+1}$ and hence this is an increase in one of the bits of the machine representation of $s_0 \cdot s$, thus $s_0 \odot s \neq s$ because they differ in at least one bit.
	For example, assume $t = 4$, $s_0 = 1.011$ and $s = 1.001$ (that is its smallest possible value). Then the product is
	\begin{center}
		\begin{tabular}{c@{\;}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c}
			& & & & 1. & 0 & 1 & 1 \\
			$\times$ & & & & 1. & 0 & 0 & 1 \\
			\hline
			& 0. & 0 & 0 & 1 & 0 & 1 & 1 \\
			$+$ & 1. & 0 & 1 & 1 & 0 & 0 & 0 \\
			\hline
			& 1. & 1 & 0 & 0 & 0 & 0 & 1
		\end{tabular}
	\end{center}
	so the result is represented (both with rounding and truncation) as $1.100$, that is different from $s_0 = 1.011$.

	In all four cases, the function has no fixpoint, and so it's acyclic. So this family meets hypothesis of theorem \ref{ch4:th:non-empt-res-local}: no abstract domain on floating-point numbers can be non emptying for all multiplications.
	Observe that theorem \ref{ch4:th:non-empt-res-local-basic} wasn't enough to prove this result, since in general these functions are not injective because of approximations. For instance both products
	\begin{center}
		\hfill
		\begin{tabular}{c@{\;}c@{\;}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c}
			& & & & & 1. & 1 & 0 & 0 \\
			$\times$ & & & & & 1. & 1 & 0 & 0 \\
			\hline
			    & & 0. & 1 & 1 & 0 & 0 & 0 & 0\\
			$+$ & & 1. & 1 & 0 & 0 & 0 & 0 & 0 \\
			\hline
			  & 1 & 0. & 0 & 1 & 0 & 0 & 0 & 0
		\end{tabular}
		\hfill
		\begin{tabular}{c@{\;}c@{\;}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c@{\,}c}
			& & & & & 1. & 1 & 0 & 1 \\
			$\times$ & & & & & 1. & 1 & 0 & 0 \\
			\hline
			    & & 0. & 1 & 1 & 0 & 1 & 0 & 0\\
			$+$ & & 1. & 1 & 0 & 1 & 0 & 0 & 0 \\
			\hline
			  & 1 & 0. & 0 & 1 & 1 & 1 & 0 & 0
		\end{tabular}
	\end{center}
	are approximated by the same floating-point number $1.001 \cdot 2^1$ but their first arguments differ by the last bit.
\end{example}

\section{Second result}
The second result we propose is instead more ``global", in the sense that it requires conditions on $F$ as a whole, and not on each $f$ on its own.

\begin{theorem}\label{ch4:th:non-empt-res-global}
	Let $F$ be a family of functions from $C$ in itself such that
	\begin{itemize}
		\item $F$ is highly surjective
		\item for all pair of elements $c, d \in C$ there exists at most a finite amount of $f \in F$ such that $f(d) = c$
		\item for all pair of an element $c \in C$ and a function $f \in F$, there exists at most a finite amount of elements $d \in C$ such that $f(d) = c$
	\end{itemize}
	Assume also that $R$ isn't empty. Then $A$ can't be non emptying for all $f \in F$.
\end{theorem}

The idea of the proof is that all these finiteness requirements makes it impossible to build enough $\bar{c}$ for which there exists $f_{\bar{c}}$ that maps them into $c_0$ (ie. $\bar{c} \in P(c_0)$), contradicting the hypothesis that $F$ is highly surjective. This may seem a little contradictory on its own (after all it is us that are requiring $F$ to satisfy all those conditions), but actually it is not because the proof also involves finiteness of $R$ and $R(c_0)$, that are enforced by the abstract domain. In fact we'll present an example of function family that meets those requirements.

The actual proof follows this idea in the opposite direction: starts from the (infinite) set $P(c_0)$ of $\bar{c}$ and propagates its infiniteness down to $R$.

\begin{proof}
	Assume by contradiction that $A$ is non emptying for all $f \in F$. By hypothesis, $R$ isn't empty and thus we can take a $c_0 \in R$.

	Since $F$ is an highly surjective family, $P(c_0)$ is infinite. Using lemma \ref{ch3:th:R-S-bound-integer-inf} to say that $R(c_0)$ is finite we can say that
	\begin{align*}
		E &= \{ \bar{c} \in C \svert \bar{c} \notin R(c_0), \exists f_{\bar{c}} \in F\ .\ f_{\bar{c}}(\bar{c}) = c_0 \} \\
		&= P(c_0) \setminus R(c_0)
	\end{align*}
	is infinite.

	Now fix a function $f \in F$, and let $J(f)$ be the set of $\bar{c}$ for which $f$ is exactly $f_{\bar{c}}$:
	\[
	J(f) = \{ \bar{c} \in C \setminus R(c_0) \svert f = f_{\bar{c}} \}
	\]

	Hence for all $\bar{c} \in J(f)$ we have $f(\bar{c}) = c_0$. By the third condition on $F$, this can be true for at most a finite amount of $\bar{c}$, that is $J(f)$ is finite.

	Now let $G$ be the set of functions in $F$ that are $f_{\bar{c}}$ for some $\bar{c} \in E$:
	\[
	G = \{ f \in F \svert \exists \bar{c} \in E\ .\ f = f_{\bar{c}} \}
	\]
	Clearly
	\[
	E = \bigcup\limits_{f \in G} J(f)
	\]
	But we know that $E$ is infinite while all $J(f)$ are finite, so $G$ must be infinite too.

	By lemma \ref{ch3:th:f-non-repr-pair}, for all $\bar{c} \in E$ we have $f_{\bar{c}}(c_0)$ is representable. This can be equivalently restated saying that for all $f \in G$, $f(c_0)$ is representable.
	So consider the set $I$ of all possible images of $c_0$ through functions in $G$:
	\[
	I = \{ f(c_0) \svert f \in G \}
	\]
	This set is a subset of $R$ because all its elements are representable.

	Clearly
	\[
	G = \bigcup\limits_{d \in I} \{ f \in G \svert f(c_0) = d \}
	\]
	But we know that $G$ is infinite and, for any $d \in C$, by the second condition on $F$ we have that
	\[
	\{ f \in F \svert f(c_0) = d \}
	\]
	is finite, and this is a superset of $\{ f \in G \svert f(c_0) = d \}$. So $I$ must be infinite too.

	However, this yields the desired contradiction: $I$ is infinite and is a subset of $R$, that is finite.
\end{proof}

As an example of such a family of functions we again propose floating-point numbers with multiplications:
\begin{example}
	Take $C = \mathcal{F} \setminus \{ 0 \}$ the set of non-zero floating-point numbers with $t$ bits significants, one bit sign and arbitrary precision exponents, and
	\[
	F = \{ \lambda x . x \odot y \svert y \in \mathcal{F} \setminus \{ 0 \} \}
	\]

	A straightforward adaptation of the argument of example \ref{ch4:ex:fp-numbers-local} (to take into account signs) shows that this family is highly surjective.
	Fixed two floating-point numbers $x, y$, we have that $y = f(x) = x \odot z$ only if
	\[
	\left\lvert \frac{y - (x \cdot z)}{x \cdot z} \right\rvert < \macheps
	\]
	that is
	\[
	\left\lvert\frac{y}{x}\right\rvert \frac{1}{1+\macheps} < \abs{z} < \left\lvert\frac{y}{x}\right\rvert \frac{1}{1-\macheps}
	\]
	This is a bounded interval since $x \neq 0$, and hence contains only a finite amount of floating-point numbers.
	Conversely, fixed a floating point $y$ and a function $f(x) = x \odot z$, we have that $y = x \odot z$ only if the same condition as above holds, that solved in $x$ becomes
	\[
	\left\lvert\frac{y}{z}\right\rvert \frac{1}{1+\macheps} < \abs{x} < \left\lvert\frac{y}{z}\right\rvert \frac{1}{1-\macheps}
	\]
	Again since $z \neq 0$ this is a bounded interval, thus proving the finiteness of the amount of floating-point $x$ that satisfies it.

	So, by means of theorem \ref{ch4:th:non-empt-res-global} above, we proved again that no abstract domain on floating-point numbers can be non emptying for all multiplications.
\end{example}

\section{Limitations}
\todo{reorganize}
The requirement that the function family $F$ is highly surjective is somewhat global: we require that \textit{all} possible $c$ has infinitely many preimages.
This is needed because first we fix the function family we want to analyse, then we can pick the abstract domain. Thinking of this as a game, player one fixes the function family and wants to show that there are no abstract domain able to analyse them, so player 2 thinks of a counterexample suited for that family. In fact, if there is even a single $c$ for which the condition doesn't hold, player 2 can actually construct such counterexample, as showed in the following proposition.
\todo{Move in another section "Limitations" with generalizations}
\begin{prop}
	For any fixed family $F$ of functions from $C$ in itself that is not highly onto, there exists an abstract domain $A_F$ such that:
	\begin{itemize}
		\item $A_F$ is finite
		\item all functions $f \in F$ are non emptying in $A_F$
	\end{itemize}
\end{prop}
\begin{proof}
	Since $F$ is not highly onto, there exists $c_0 \in C$ such that $P(c_0)$ is finite. We then define $A_F$ as follows.
	
	The only element of $C$ representable on its own is $c_0$ itself, ie. $R = \{ c_0 \}$.
	A pair of elements of $C$ is representable if and only if one of its elements is $c_0$ and the other is in $P(c_0)$. This also means that $R(c_0) = P(c_0)$.
	Subsets of $C$ with at least three elements are representable if and only if they are unions of representable pairs.
	
	Putting all together
	\[
	A_F = \{ \emptyset, \{ c_0 \} \} \cup \{ \{ c_0 \} \cup T \svert T \subseteq P(c_0) \}
	\]
	
	$A_F$ is a Moore family with respect to $\pow(C)^{\op}$ (it contains the maximal element, that is $\emptyset$, and is closed by union, that is meet in the opposite powerset), hence $A_F$ is a correct under-approximation abstract domain.
	
	Since $R(c_0) = P(c_0)$ is finite, we get that $A_F$ is finite too:
	\[
	\abs{A_F} = 2 + 2^{\abs{P(c_0)}}
	\]
	
	Now we want to show that any $f$ in $F$ is non emptying in $A_F$.
	We first observe that a subset $S \subseteq C$ is such that $\alpha(S) \neq \emptyset$ if and only if $c_0 \in S$. Suppose that $c_0 \in S$, then
	\[
	\alpha(S) \supseteq \alpha(\{ c_0 \}) = \{ c_0 \} \supset \emptyset
	\]
	Conversely, suppose that $\alpha(S) \neq \emptyset$. Since all elements of $A_F$ but the empty set contains $c_0$, by correctness we have
	\[
	c_0 \in \alpha(S) \subseteq S
	\]
	
	So fix now $S \subseteq C$ an element of the concrete domain. If $\alpha(S) = \emptyset$ then the non emptying condition is vacuously true, so assume $\alpha(S) \neq \emptyset$, or equivalently that $c_0 \in S$.
	Consider now $\alpha(f(S))$. If this is empty again the non emptying condition is vacuously true, so assume it is not. Again, this holds if and only if $c_0 \in f(S)$, that can be rewritten as $\exists d \in S\ .\ f(d) = c_0$. But by definition of $A_F$ we know this is equivalent to $d \in P(c_0) = R(c_0)$.
	Hence
	\begin{align*}
		&S \supseteq \{ c_0, d \} \\
		\implies& \alpha(S) \supseteq \alpha(\{ c_0, d \}) = \{ c_0, d \} \\
		\implies& f(\alpha(S)) \supseteq f(\{ c_0, d \}) \supseteq \{ f(d) \} = \{ c_0 \} \\
		\implies& f^{\flat}(\alpha(S)) = \alpha(f(\alpha(S))) \supseteq \alpha(\{ c_0 \}) = \{ c_0 \}
	\end{align*}
	Lines are motivated respectively by: monotonicity of $\alpha$ and representability of $\{ c_0, d \}$; monotonicity of $f$ and the fact that $f(d) = c_0$; monotonicity of $\alpha$ and representability of $\{ c_0 \}$. Also note that the implication chain above holds even if $d = c_0$.
	The last line implies $f^{\flat}(\alpha(S)) \neq \emptyset$, so $f$ is non emptying in $A_F$.
\end{proof}

We now show a simple example of the construction used in the proof
\begin{example}
	Fix the pair of functions $f(x) = x - 1$ and $g(x) = x - 2$ on $\setZ$, and let us build an under-approximating abstract domain for which these functions are non emptying. Following the proof of the previous proposition, we need an integer $n_0$ such that only a finite amount of integers can be mapped into it using either $f$ or $g$. Clearly any integer is fine, so let us fix $n_0 = 0$.
	
	The set $P(0)$ of integers that can be mapped into $0$ is simply $P(0) = \{ 1, 2 \}$. The abstract domain is then
	\[
	A_F = \{ \emptyset, \{ 0 \}, \{ 0, 1 \}, \{ 0, 2 \}, \{ 0, 1, 2 \} \}
	\]
	Both $f$ and $g$ are non emptying in $A_F$. A set $S$ isn't abstracted to $\emptyset$ if and only if it contains $0$, so fix one that does.
	For $f(S)$ not to be abstracted to $\emptyset$ we need also it to contain $0$, that means $1 \in S$. So
	\begin{align*}
		f^{\flat}(\alpha(S)) &= \alpha(f(\alpha(S))) \\
		&\supseteq \alpha(f(\alpha(\{ 0, 1 \}))) \\
		&= \alpha(f(\{ 0, 1 \})) \\
		&= \alpha(\{ -1, 0 \}) = \{ 0 \}
	\end{align*}
	The check for $g$ is analogous.
\end{example}

This proposition gives us one limit of approaches based on non emptying functions to show non existence of under-approximating abstract domains: whatever we do, we must fix enough functions to have high ontoness. This in particular means those approaches are ill suited to prove results where we focus on a single function.

In the following sections, we present two impossibility results. They share the same general structure: we fix a function family $F$ with some properties, assume that they are all non emptying in the abstract domain and that $R$ isn't empty. We need this last assumption in order to use lemma \ref{ch3:th:f-non-repr-pair}: if $R$ is empty we can't prove $f(\bar{c}) \in R$.

\todo{Scrivere bene le generalizzazioni e un po' di contesto}
Generalization of the above prop.
\begin{prop}
	Fix a family $F$ of functions from $\pow(C)$ in itself, and assume there is a set $S \subseteq C$ such that $P(S)$ is finite. Then there exists a finite abstract domain $A_F$ such that all functions $f \in F$ are non emptying in $A_F$.
\end{prop}
The proof is more or less the same.

``Dual" result of this (the other goes ``along" $F$, this goes ``backward").
\begin{prop}
	Fix a family $F$ of functions from $\pow(C)$ in itself, and for $S \subseteq C$ let $I(S)$ the set of all iterate images of $S$, that is the smallest subset of $\pow(C)$ such that
	\begin{align*}
		&S \in I(S) \\
		&f(T) \in I(S) \text{ for all } f \in F \text{, } T \in I(S)\\
	\end{align*}
	Assume there is a set $S \subseteq C$ such that $I(S)$ is finite. Then there exists a finite abstract domain $A_F$ such that all functions $f \in F$ are non emptying in $A_F$.
\end{prop}
The set $I(S)$ is just obtained from $S$ applying any possible sequence of functions in $F$ to it.
