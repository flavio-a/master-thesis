\chapter{Over versus under-approximations}
In this chapter we compare over and under-approximation. We would like to describe the many similarities but more importantly the basic differences between the two, with a preferential view on abstract interpretation.

As anticipated in the introduction, over and under-approximations are designed with different, dual goals in mind.
Over-approximations can prove correctness of a program, that is the absence of bugs: since it computes a superset of all the behaviours of a program, whenever all of these satisfy the specification, also real behaviours, that are a subset, do so. However, they are ill suited to find bugs, since the behaviour violating the specification may be part of the over-approximation but not of the program itself (these are called false alarms).
Conversely, under-approximations can prove incorrectness of a program, or presence of bugs: whenever one of the identified behaviours violate the specification, that is a possible execution of the program that then have a bug. On the other hand, under-approximation can't prove the correctness of a program.

In the last few decades, the study of formal static analysis focused mainly on over-approximation. However, recently O'Hearn \cite{ohearn-incorrectness-logic} stressed the practical importance of under-approximation and proposed to study it in a principled way, as it has been done for over-approximation.
In his work he proposes ``incorrectness logic"\footnote{Actually, the technical development of incorrectness logic is due to de Vries and Koutavas \cite{de-vries-koutavas-reverse-hoare-logic}, who developed it before O'Hearn, but they proposed it for the specification of randomized algorithm. O'Hearn re-developed it independently and recognized its application in bug catching}, a dual version of Hoare logic thought from the ground up for under-approximation in order to find bugs. There are many similarities between the two, but the single most important difference is the consequence rule, where implications are reversed in under-approximation with respect to classic Hoare logic. In over-approximation, we're allowed to weaken the post-condition, enlarging the set of final states because it remains a sound over-approximation; in under-approximation we can strengthen it, reducing possible final states and hence still guaranteeing the under-approximation.
Another, possibly more striking example of this duality is what O'Hearn calls $\land \lor$ symmetry. If in over-approximation the final result satisfies both $q_1$ and $q_2$, it satisfies $q_1 \land q_2$. Dually, if in under-approximation the final result contains both $q_1$ and $q_2$ then it must contain $q_1 \lor q_2$. While in over-approximation we carry over the logical conjunction of triples to the formulas, in under-approximation this is dualized into a logical disjunction.

This very same duality is present also in abstract interpretation: we observed that under-approximation is an order theoretic opposite of over-approximation, so that we just replace smaller with greater (direction of implications) and lubs with glbs (logical disjunctions and conjunctions).
This seems to  allow a direct translation of ``standard" abstract interpretation theory to under-approximation, and is probably the reason that motivated the original authors to state that it could be applied to both over and under-approximation. However, as we try to show in the remainder of the chapter, there are two main points that aren't dualized in abstract interpretation, breaking the (only apparent) symmetry.

\section{Functions}
Concrete functions are not dualized. Their abstract semantics is different since $\alpha$ and $\gamma$ changes, but the functions themselves on the concrete domain doesn't change, and this is an asymmetry on its own.

In general this is not a problem at the level of concrete \textit{values}, since they are regarded as flat from the abstraction, but manifests itself when we lift functions to \textit{set} of values as additive extensions.
As an example, consider the domain of integers $\setZ$. We observed before that the concrete domain $C$ on which the analysis is performed is not $\setZ$ itself, but $\pow(\setZ)$ because we're interested in set of values. In this case, we consider additive extensions of basic transfer functions on $\setZ$, and those impose a ``direction" on $C = \pow(\setZ)$, simply by an argument of cardinality.
The additive extension of sum on a pair of non empty sets $S, T \subseteq \setZ$ satisfies
\[
\abs{S + T} \ge \max\{ \abs{S}, \abs{T} \}
\]
If both $T$ and $S$ contains an element different than $0$ the same holds also for product.
Conversely, an initialization assignment like \code{x := 10} has an output cardinality of $1$ irrespective of the input, unless this is the empty set (that, as we'll discuss in the next section, describes divergence and introduces problems on its own).

This ``direction" of functions influences abstract domains design because it introduces an asymmetry in ``interesting sets" of the concrete domain.
If we didn't consider those, we could just define an under-approximation domain from any over-approximation one taking complements, as done in \cite{lev-backward-analysis-complement}.
\begin{example}
	Intervals (see example \ref{ch2:ex:intervals}) can be made an under-approximation domain by taking as meaning of an abstract element the complement of the set it represents as an over-approximation.
	For instance, the interval $[n, m]$ would represent the set of elements \textit{not} in the interval, ie. $\{ x \in \setZ \svert x < n \lor x > m \}$.
	$\alpha$ and $\gamma$ are changed accordingly.

	This is a correct under-approximation domain: complementation reverse ordering, so it de facto changes $A$ into $A^{\op}$. However in practice it isn't useful because some basic transfer functions doesn't behave well with the sets described, for instance initializations.
\end{example}

An alternative way to see this very same asymmetry is the fact that interesting sets, determined by functions, are (more or less) the same both in over and under-approximation since the basic transfer functions are the same, but in the former we must ensure the domain is closed under intersection, in the latter under union. This forces us to put in the abstract domain only some interesting set, in order to ensure closure by the required operation without making it too large to be feasible for the analysis, and this of course deeply depends on which operation the domain must be closed under.

This point of view also shows why incorrectness logic doesn't suffer from this issue. Logic is intrinsically closed under both union and intersection, because these correspond to logical disjunction and conjunction, the payback being that formulas at hand become more complex. However, using incorrectness logic's consequence rule, it is possible to simplify formulas by dropping disjunctions, losing precision to gain speed, in an application of O'Hearn's sentence \cite{ohearn-incorrectness-logic} ``For incorrectness reasoning, you must remember information as you go along a path, but you get to forget some of the paths".
An abstract interpretation equivalent of this approach would be to use the disjunctive completion of well-known abstract domains \cite{under-approx-disjunctive-completion}, that correspond to take the exact union of abstract elements. The disjunctive completion of a domain is formally its power set, where each set of abstract element represent the union of their concretizations. For instance, in the disjunctive completion of the interval domain, the set
\[
\{ [0, 10], [20, 30], [100, +\infty] \}
\]
represents
\[
\{ 0, 1, \dots, 10, 20, 21, \dots, 30, 100, 101, \dots \}
\]
the union of these three intervals. Clearly a disjunctive completion is closed by union and hence is a sound under-approximation domain, however in general it can (and does) lead to an exponential explosion of the number of disjunctions during the analysis, making it unfeasible. Luckily, for under-approximation it's safe to drop disjunctions in abstract interpretation as it is in logic: if all the states described before were reachable, also dropping one of the disjunctions, that is removing some values from the concretization, yields only reachable states. This allows to limit complexity at the expense of precision, but raises the issue of \textit{which} disjunctions to drop and \textit{when}, that should be handled by heuristics. A possibility would be to take inspiration from logic, further investigation may lead to interesting results.

\section{Divergence}
In over-approximation, the abstract $\top$ represent absence of information: any set of concrete values is correctly described by $\top$ because it's always the case that $\alpha(c) \preceq \top$. This role in under-approximation is, dually, filled by $\bot$ since all values satisfy $\bot \preceq \alpha(c)$.
However the concrete $\emptyset$, that is always the concretization of $\bot$ in under-approximation, represents also divergence, the fact that a certain program point cannot be reached by any concrete state, and this is not dualized moving from over to under-approximation.

This is an issue because many concrete functions are strict: if their input diverges, their output does too. There are exceptions (for instance in some functional languages), but most programming languages evaluate arguments of a function before calling it, making them strict. Moreover, strictness is carried over in under-approximation abstractions:
\[
f^{\flat}(\bot) = f^{\flat}(\alpha(\emptyset)) \preceq \alpha(f(\emptyset)) = \alpha(\emptyset) = \bot
\]
Together, these two facts implies that when the abstract analysis determines $\bot$ for a program point, it will almost always determine $\bot$ for all points reachable from that one, at least until a join in the control flow of the program (eg. at the end of an \code{if}, where the \code{then} and \code{else} branches merge together). We may colloquially say that ``recovery" from $\bot$, that means producing a meaningful analysis result starting from it, is very hard.
In over-approximation the issue is much lighter: ``recovery" from $\top$ is quite common, even though it often introduces severe loss of precision and so it should be avoided as much as possible.

This has heavy repercussions on abstract domain design. In over-approximation we can admit interesting sets without an abstraction, because we can use $\top$ hoping in a recovery afterwards, in order to have a feasible domain. In under-approximation instead, any set that is abstracted to $\bot$ is a set that, whenever reached, removes any possible usefulness to the analysis.
Moreover, sets that are easily abstracted with $\bot$ in under-approximation are small sets, especially singletons, that are common during program executions, for instance at initializations or beginning of loops.

Another situation in which $\bot$ may arise in under-approximation analysis is when conditional are ``incompatible" with the abstract domain, that is each abstract element contains both concrete values that does satisfy the condition and that doesn't.
For instance, a numerical abstract domain can't answer a question about the memory layout, like whether a certain pointer is the last of a list or not: a guard like \code{l->next == NULL} could be satisfied both by states where \code{l->val} is within the interval $[0, 10]$ and where it isn't. So, by correctness of the abstraction, no interval (or octagon, or polyhedron) can safely under-approximate the set of states that satisfy the guard, thus leading to $\bot$ as the only sound possibility.
In a tool based on logic, the analyser must add the guard in conjunction with the formula, making this an issue of complexity and so ultimately of performances, but as long as the guard can be described within the logic it doesn't invalidate the analysis completely; other components of the tool can improve performances applying safe simplifications to the formula (eg. dropping disjunctions).
Abstract interpretation doesn't have this freedom because it has to fix the set of formulas it can represent beforehand, in the form of the choice of the abstract domain. This fact is an abstract interpretation view on O'Hearn's sentence \cite{ohearn-incorrectness-logic} ``For incorrectness reasoning, you must remember information as you go along a path, but you get to forget some of the paths": whenever you can't remember all information, you have to forget the path entirely.
